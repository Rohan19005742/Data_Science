{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccc515f",
   "metadata": {},
   "source": [
    "# Regularisation in Machine Learning\n",
    "Regularization is a technique used in machine learning to prevent overfitting and improve performance on unseen data.\n",
    "\n",
    "There are 3 types of regularisation:\n",
    "1. Lasso (l1)\n",
    "2. Ridge\n",
    "3. Elastic Net\n",
    "\n",
    "Each type applies penalties in different ways to control model complexity and improve generalisation.\n",
    "\n",
    "Note: Linear Regression fits all features equally, minimizing squared error.\n",
    "\n",
    "## L1 (Lasso) Regularisation\n",
    "\n",
    "- It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function (MSE). This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less important ones.\n",
    "\n",
    "cost = MSE + λ*∑​∣wi​∣ (absolute sum of all model weights)\n",
    "\n",
    "The hyperparameter λ is the strength of penalisation. \n",
    "- Higher = stronger regularization (more coefficients pushed toward 0)\n",
    "- Lower = weaker regularization (model fits data more closely, but higher risk of overfitting)\n",
    "\n",
    "Use cross validation of λ to find the best balance between bias and variance.\n",
    "\n",
    "- L1 can be used for feature selection as it pushes some features to 0 coefficient.\n",
    "\n",
    "- Note: it encourages some weights to be 0, effectively performing feature selection.\n",
    "- Geometrically, the constraint forms a diamond shape.\n",
    "- When the optimization boundary touches the diamond corners, some weights become exactly 0 → sparse solution.\n",
    "- Result: Feature selection — only the most important features remain.\n",
    "\n",
    "- Disadvantages:\n",
    "    - can underperform when all features are relevant.\n",
    "    - unstable when featurs are correlated (tends to pick 1 of the 2 correlated features)\n",
    "\n",
    "## L2 (Ridge) Regularisation\n",
    "\n",
    "- It sums the squares of the weight coefficients.\n",
    "    - this causes all weights of coefficients to shrink\n",
    "\n",
    "cost = MSE + λ*∑​(wi​)^2 (sum of all model weights squared)\n",
    "\n",
    "- Encourages small weights (shrinks them toward zero), but rarely makes them exactly zero.\n",
    "\n",
    "- Geometrically, the constraint forms a circle (or sphere).\n",
    "- Optimization tends to shrink all weights smoothly, none drop exactly to zero.\n",
    "- Result: Keeps all features, but reduces their magnitude (weight shrinkage).\n",
    "\n",
    "- Disadvantages:\n",
    "    - no feature selection\n",
    "    - less interpretable model, harder to know which features matter\n",
    "    - not ideal when you have many irrelevant variables\n",
    "    - assumes normal data distribution\n",
    "    - may not handle spare data, unlike l1\n",
    "\n",
    "\n",
    "## Elastic Net\n",
    "- combines l1 and l2\n",
    "\n",
    "\n",
    "cost = MSE + λ1​∑​∣wi​∣+λ2​∑​(wi)^2\n",
    "\n",
    "\n",
    "\n",
    "## When to use\n",
    "\n",
    "- Many irrelevant or redundant features:\tL1 (Lasso) — will eliminate them\n",
    "- All features are useful but may be noisy:\tL2 (Ridge) — reduces overfitting without discarding\n",
    "- Highly correlated features:\tL2 (Ridge) tends to share weight between them\n",
    "- Need interpretability (which features matter):\tL1 (Lasso)\n",
    "- Numerical stability & smooth optimization:\tL2 (Ridge)\n",
    "- Want both benefits:\tElastic Net (mix of L1 and L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d87760",
   "metadata": {},
   "source": [
    "## Comparion of each reguarisation and base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41e63a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chevrolet chevelle malibu</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick skylark 320</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plymouth satellite</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amc rebel sst</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford torino</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mpg  cylinders  displacement  horsepower  weight  \\\n",
       "name                                                                           \n",
       "chevrolet chevelle malibu  18.0          8         307.0         130    3504   \n",
       "buick skylark 320          15.0          8         350.0         165    3693   \n",
       "plymouth satellite         18.0          8         318.0         150    3436   \n",
       "amc rebel sst              16.0          8         304.0         150    3433   \n",
       "ford torino                17.0          8         302.0         140    3449   \n",
       "\n",
       "                           acceleration  year  origin  \n",
       "name                                                   \n",
       "chevrolet chevelle malibu          12.0    70       1  \n",
       "buick skylark 320                  11.5    70       1  \n",
       "plymouth satellite                 11.0    70       1  \n",
       "amc rebel sst                      12.0    70       1  \n",
       "ford torino                        10.5    70       1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('../../data/Auto.csv')\n",
    "data.set_index('name', inplace=True)\n",
    "data_X = data.drop('mpg', axis=1)\n",
    "data_y = data['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.2, random_state=42)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ea5cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.34578883  0.01510871 -0.02130175 -0.00614163  0.03795001  0.76774258\n",
      "  1.61345707]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7901500386760352"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "r_score = model.score(X_test, y_test)\n",
    "coefficients = model.coef_\n",
    "print(\"Coefficients:\", coefficients)\n",
    "r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b58c40b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789304646084754"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 Regularisation (Lasso)\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "lasso_r_score = lasso_model.score(X_test, y_test)\n",
    "lasso_r_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f669d",
   "metadata": {},
   "source": [
    "We can see that L1 has not improved the r^2 in this case, lets try to cross validate and find the best alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f9d1620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.49\n",
      "Best score: 0.7933732960042428\n",
      "Coefficients: [-0.         -0.         -0.01122415 -0.00630222  0.          0.71450716\n",
      "  0.26975546]\n"
     ]
    }
   ],
   "source": [
    "# L1 Regularisation (Lasso) cross validation\n",
    "\n",
    "parameters = {'alpha': [i for i in np.arange(0.01, 1.0, 0.01)]}\n",
    "scores = []\n",
    "for parameter in parameters['alpha']:\n",
    "    model = Lasso(alpha=parameter)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "best_alpha = parameters['alpha'][np.argmax(scores)]\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Best score:\", max(scores))\n",
    "coefficients = Lasso(alpha=best_alpha).fit(X_train, y_train).coef_\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0adad",
   "metadata": {},
   "source": [
    "alpha of 0.49 gives us the best score and this is slightly higher than the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1486b238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7901661107227567"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l2 Regularisation (Ridge)\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_r_score = ridge_model.score(X_test, y_test)\n",
    "ridge_r_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce74a0",
   "metadata": {},
   "source": [
    "l2 gives a slightly better score than base modle for alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e378fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.99\n",
      "Best score: 0.7903067847624526\n",
      "Coefficients: [-0.33971326  0.01492141 -0.02114254 -0.00614542  0.03810602  0.76745057\n",
      "  1.59935737]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {'alpha': [i for i in np.arange(0.01, 1.0, 0.01)]}\n",
    "scores = []\n",
    "for parameter in parameters['alpha']:\n",
    "    model = Ridge(alpha=parameter)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "best_alpha = parameters['alpha'][np.argmax(scores)]\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Best score:\", max(scores))\n",
    "coefficients = Ridge(alpha=best_alpha).fit(X_train, y_train).coef_\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d37d7a",
   "metadata": {},
   "source": [
    "after using cross validation, it seems l1 gives the best result compared to l2 and base. This could be due to the fact that there are irrelevant feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
