{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328aaefd",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "- Involves training the model with labeled data. The model attempts to find the mapping between the input (features) X and output (label, target) Y.\n",
    "- The model then attemps to predict or classify on unseen data.\n",
    "\n",
    "#### Objectives of supervised learning\n",
    "- Accuratly predict on unseen test cases.\n",
    "- Understand which inputs affects the output and how.\n",
    "- Assess the quality of our predictions.\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Involes training the model with unlabeled data. The model attemps to find patterns and relationships in the data. There is no output Y.\n",
    "- The model then attemps to cluster unseen data based on similarity with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fc382",
   "metadata": {},
   "source": [
    "### Regression function\n",
    "f(x) = E(Y | X = x)\n",
    "\n",
    "- Typically we have few/no data points for all x, so we cannot compute E(Y | X = x)\n",
    "\n",
    "\n",
    "![](../images/regression1.png)\n",
    "\n",
    "- What we can do is relax the definition and let ^f(x) = Average(Y | X in Neightbour(x))\n",
    "- we take some of the neighbours of x\n",
    "\n",
    "Nearest neighbour (NN) average can be pretty good for small number of features (p) and large number of data points.\n",
    "NN can be lousy when p is large: Curse of dimensionality.\n",
    "\n",
    "We can instead use linear model, although it is almost never correct. It gives a reasionable fit.\n",
    "\n",
    "\n",
    "#### Trade Offs\n",
    "\n",
    "- Prediciton accuracy vs interpretability\n",
    "\n",
    "- Good fit vs Over fit vs Under fit\n",
    "\n",
    "- Parsimoney values vs black-box\n",
    "  - simple models (fewer varibales) vs complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08dfcd",
   "metadata": {},
   "source": [
    "### Assessing Model Accuray\n",
    "\n",
    "- MSE: \n",
    "  - = 1/n * sum from i=1 to n (yi - ^f(xi))^2\n",
    "  - penalises outliers more (bias towards overfit function)\n",
    "  - Used in gradient descent to optimise the model\n",
    "  - Not very interpretable, say you are prediciting house prices, MSE units will be dollars squared which is not meaningful.\n",
    "\n",
    "- MAE\n",
    "  - = 1/n * sum from i=1 to n (|yi - ^f(xi)|)\n",
    "  - more interpretable as units are the same as your data\n",
    "  - less sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc6998",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-off\n",
    "\n",
    "- Bias: The error made due to the underlying assumption. (Bias is an error from a model's oversimplified assumptions, causing it to consistently miss the \"true\" values (underfitting))\n",
    "  - Bias(^Y)=E(^Y)−Y\n",
    "\n",
    "- Variance: How sensitive the model is to change in training data. (Variance refers to the amount by which fˆ would change if we\n",
    "estimated it using a different training data set.) (High variance = Overfitting)\n",
    "  - Variance=E[(^Y−E[^Y])^2]\n",
    "\n",
    "- Flexibilty: Number of parameters of the model.\n",
    "  - As flexibilty increases, the complexity increases but interpretability decreases.\n",
    "\n",
    "\n",
    "Typically as flexibilty of model increases, the variance increases and bias decreases. Choosing the flexibilty based on average test error is the bias-variance trade-off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
