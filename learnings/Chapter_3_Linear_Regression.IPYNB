{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048dcc5e",
   "metadata": {},
   "source": [
    "# Chapter 3 - Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688906",
   "metadata": {},
   "source": [
    "## What is linear Regression\n",
    "- Simple supervised learning method\n",
    "- Linear Regression assumes that the dependency of Y on predictors X is linear.\n",
    "- We assume a model: Y = intercept+slope*X + error term\n",
    "- with linear data the mean of error term = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b08ce",
   "metadata": {},
   "source": [
    "## Questions we want answered\n",
    "\n",
    "- Is there a relationship between X and Y?\n",
    "- How strong is the relationship?\n",
    "- Which X contribute most to Y?\n",
    "- How accurately can we predict future values of Y?\n",
    "- Is the relationship linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68381e",
   "metadata": {},
   "source": [
    "## Estimation of parameters by least squares\n",
    "\n",
    "- let ^y be the prediciton for Y \n",
    "- Then ei = yi - ^yi which is the ith residual.\n",
    "- We define the residual sum of squares (RSS) as e1^2 + e2^2 + ... + en^2\n",
    "\n",
    "\n",
    "- The least squares approach chooses intercept and slope that minimise the RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8940ced",
   "metadata": {},
   "source": [
    "## Confidence Interval Of Model Parameters\n",
    "let Y = Ax + B, be the true model\n",
    "let y = ax + b, be the estimate model for Y\n",
    "\n",
    "With different data sets we will get a new regression line each time.\n",
    "So how do we know which one would be best sutible for unseen data?\n",
    "\n",
    "we can find the 95% interval for the true model by adding 2 SD to the mean.\n",
    "a +- 2*SD(a)\n",
    "b +- 2*SD(b)\n",
    "\n",
    "With 95% confidence we can say that the true values of a and b will be in this range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9628a94",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "h0: There is no relationship between X and Y.\n",
    "\n",
    "h1: There is a relationship between X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9f238",
   "metadata": {},
   "source": [
    "## Assessing the Overall Accuray of the Model\n",
    "\n",
    "Residual sum of squares (RSS): sum of i=1 to n (yi - ^yi)^2\n",
    "  - How much variation is left unexplained by your model\n",
    "\n",
    "Residual standard Error (RSE): root(RSS/n-2)\n",
    "  - The average amount by which the observed values differ from the regression line (typical error size)\n",
    "  - Why n-2? because we estimated 2 parameters.\n",
    "  - Small RSE, points are close to the regression line \n",
    "  - This is an estimate of the SD of e (irreducible error)\n",
    "\n",
    "Total sum of squares (TSS): sum of i=1 to n (yi- _y)^2\n",
    "  - Where _y is the mean of the y\n",
    "  - How much variation is in the response variable Y before considering the model.\n",
    "  \n",
    "R-squared (R^2): (TSS - RSS) / TSS\n",
    "  - Statistical measure of how well your model fits the data\n",
    "  - How much of the total variation is explained by the model.\n",
    "\n",
    "\n",
    "### Notes\n",
    "- e.g r-squared = 0.9: The model through the predictors, explains 90% of the variability in Y.\n",
    "- TSS - RSS is the drop in training error, bigger the drop the better the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cb653",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "- Y = a bX1 + cX2 + dX3 + ..... + e\n",
    "\n",
    "### Estimating model for multiple regression\n",
    "\n",
    "- We minimise RSS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933c20d",
   "metadata": {},
   "source": [
    "## More Quesitions\n",
    "\n",
    "1. Is atleast 1 predictor useful?\n",
    "    - We use F-statistic: F = ((TSS - RSS) / p) / (RSS / (n-p-1))\n",
    "    - p is the numner of parameters (coefficients of x, intercept is not counted in this)\n",
    "    - n is the sample size\n",
    "    - F = explained variance per predictor / unexplained variance per observation\n",
    "    - If F is close to 1 then it is likely that no predictors are useful.\n",
    "\n",
    "2. Deciding on Important Variables\n",
    "    - Forward selection\n",
    "        - Begin with null model (a model that contains y intercept but no predictors)\n",
    "        - We then fit simple linear regression for each p and add to the null model the variable that results in the lowest RSS. \n",
    "        - Then add to the model the variable that results in the lowest RSS amongst all two-variable models.\n",
    "        - Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold.\n",
    "    - Backward selection\n",
    "        - Begin with all variables in the model.\n",
    "        - Remove the variable with the largest p-value - that is the variable that is the least statistically significant.\n",
    "        - Continue until a stopping rule is reached.\n",
    "        - This cant be used if p > n because the full least squares model cannot be fit. While forward can always be used.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
