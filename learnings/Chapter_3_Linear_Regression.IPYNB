{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048dcc5e",
   "metadata": {},
   "source": [
    "# Chapter 3 - Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688906",
   "metadata": {},
   "source": [
    "## What is linear Regression\n",
    "- Simple supervised learning method\n",
    "- Linear Regression assumes that the dependency of Y on predictors X is linear.\n",
    "- We assume a model: Y = intercept+slope*X + error term\n",
    "- with linear data the mean of error term = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b08ce",
   "metadata": {},
   "source": [
    "## Questions we want answered\n",
    "\n",
    "- Is there a relationship between X and Y?\n",
    "- How strong is the relationship?\n",
    "- Which X contribute most to Y?\n",
    "- How accurately can we predict future values of Y?\n",
    "- Is the relationship linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68381e",
   "metadata": {},
   "source": [
    "## Estimation of parameters by least squares\n",
    "\n",
    "- let ^y be the prediciton for Y \n",
    "- Then ei = yi - ^yi which is the ith residual.\n",
    "- We define the residual sum of squares (RSS) as e1^2 + e2^2 + ... + en^2\n",
    "\n",
    "\n",
    "- The least squares approach chooses intercept and slope that minimise the RSS."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
