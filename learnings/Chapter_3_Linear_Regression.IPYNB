{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048dcc5e",
   "metadata": {},
   "source": [
    "# Chapter 3 - Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688906",
   "metadata": {},
   "source": [
    "## What is linear Regression\n",
    "- Simple supervised learning method\n",
    "- Linear Regression assumes that the dependency of Y on predictors X is linear.\n",
    "- We assume a model: Y = intercept+slope*X + error term\n",
    "- with linear data the mean of error term = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b08ce",
   "metadata": {},
   "source": [
    "## Questions we want answered\n",
    "\n",
    "- Is there a relationship between X and Y?\n",
    "- How strong is the relationship?\n",
    "- Which X contribute most to Y?\n",
    "- How accurately can we predict future values of Y?\n",
    "- Is the relationship linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68381e",
   "metadata": {},
   "source": [
    "## Estimation of parameters by least squares\n",
    "\n",
    "- let ^y be the prediciton for Y \n",
    "- Then ei = yi - ^yi which is the ith residual.\n",
    "- We define the residual sum of squares (RSS) as e1^2 + e2^2 + ... + en^2\n",
    "\n",
    "\n",
    "- The least squares approach chooses intercept and slope that minimise the RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8940ced",
   "metadata": {},
   "source": [
    "## Confidence Interval Of Model Parameters\n",
    "Note Y = f(x) + e is the real model\n",
    "let f(x) = Ax + B, be the true f(x) model\n",
    "let y = ax + b, be the estimate model for f(x)\n",
    "\n",
    "With different data sets we will get a new regression line each time.\n",
    "So how do we know which one would be best sutible for unseen data?\n",
    "\n",
    "we can find the 95% interval for the true model by adding 2 SD to the mean.\n",
    "a +- 2*SD(a)\n",
    "b +- 2*SD(b)\n",
    "\n",
    "With 95% confidence we can say that the true values of a and b will be in this range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9628a94",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "h0: There is no relationship between X and Y.\n",
    "\n",
    "h1: There is a relationship between X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9f238",
   "metadata": {},
   "source": [
    "## Assessing the Overall Accuray of the Model\n",
    "\n",
    "Residual sum of squares (RSS): sum of i=1 to n (yi - ^yi)^2\n",
    "  - How much variation is left unexplained by your model\n",
    "\n",
    "Residual standard Error (RSE): root(RSS/n-2)\n",
    "  - The average amount by which the observed values differ from the regression line (typical error size)\n",
    "  - Why n-2? because we estimated 2 parameters.\n",
    "  - Small RSE, points are close to the regression line \n",
    "  - This is an estimate of the SD of e (irreducible error)\n",
    "\n",
    "Total sum of squares (TSS): sum of i=1 to n (yi- _y)^2\n",
    "  - Where _y is the mean of the y\n",
    "  - How much variation is in the response variable Y before considering the model.\n",
    "  \n",
    "R-squared (R^2): (TSS - RSS) / TSS\n",
    "  - Statistical measure of how well your model fits the data\n",
    "  - How much of the total variation is explained by the model.\n",
    "\n",
    "\n",
    "### Notes\n",
    "- e.g r-squared = 0.9: The model through the predictors, explains 90% of the variability in Y.\n",
    "- TSS - RSS is the drop in training error, bigger the drop the better the model.\n",
    "- If the p value is high say over 0.05 then there is a chance that h0 is true. The smaller the p the more chances of rejecting h0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cb653",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "- Y = a bX1 + cX2 + dX3 + ..... + e\n",
    "\n",
    "### Estimating model for multiple regression\n",
    "\n",
    "- We minimise RSS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933c20d",
   "metadata": {},
   "source": [
    "## More Quesitions\n",
    "\n",
    "1. Is atleast 1 predictor useful?\n",
    "    - We use F-statistic: F = ((TSS - RSS) / p) / (RSS / (n-p-1))\n",
    "    - p is the numner of parameters (coefficients of x, intercept is not counted in this)\n",
    "    - n is the sample size\n",
    "    - F = explained variance per predictor / unexplained variance per observation\n",
    "    - If F is close to 1 then it is likely that no predictors are useful.\n",
    "\n",
    "2. Deciding on Important Variables\n",
    "    - Forward selection\n",
    "        - Begin with null model (a model that contains y intercept but no predictors)\n",
    "        - We then fit simple linear regression for each p and add to the null model the variable that results in the lowest RSS. \n",
    "        - Then add to the model the variable that results in the lowest RSS amongst all two-variable models.\n",
    "        - Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold.\n",
    "    - Backward selection\n",
    "        - Begin with all variables in the model.\n",
    "        - Remove the variable with the largest p-value - that is the variable that is the least statistically significant.\n",
    "        - Continue until a stopping rule is reached.\n",
    "        - This cant be used if p > n because the full least squares model cannot be fit. While forward can always be used.\n",
    "\n",
    "3. Model Fit\n",
    "    - RSE and R-sqaured are the most common measures of model fit.\n",
    "    - In simple linear regression R^2 is the square of the correlation of the response and variable.\n",
    "    - In multiple linear regression R^2 is = Cor(Y,^Y)^2 (the square of correlation between the response and the fitted model.)\n",
    "    - R^2 will always increase on training data with more variables as the RSS always increases. (tho this may not be the case for test data)\n",
    "\n",
    "4. Predictions \n",
    "    - Y = f(X) + e\n",
    "    - we can find the 95% confidence interval of the true value of f(X)\n",
    "    - we can also find the 95% prediction interval of the true value of Y. This inlcudes both the error in estimate of f(X) and irreducible noise which is why the range is bigger than confidence interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fae5dd",
   "metadata": {},
   "source": [
    "## Qualitative Predictors\n",
    "\n",
    "- There can be cases where we have qualitative predictors.\n",
    "\n",
    "- If the qualitative predictor has only 2 values (i.e boolean)\n",
    "    - We can transform it into a quantitative variable using 1s and 0s.\n",
    "    ![](../images/qualitative_predictors.png)\n",
    "\n",
    "- What if the predictor has more than 2 values?\n",
    "    - create another variable\n",
    "    - x1 = 1 if asian 0 if not asian\n",
    "    - x2 = 1 if african 0 if not african\n",
    "    - if not x1 or x2 then american\n",
    "    - with k levels, we create k - 1 dummy variables\n",
    "    - The base line is the value without the variable, in this case it is american. The baseline will have value of the intercept.\n",
    "    - comparisons will be made against the baseline. \n",
    "    - The choice of the baseline does not matter as RSS will be the same, however the contrasts you make will change (p value will also change). e.g asian is +10 to american. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5deb98",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "- The relationship between the response and predictors are additive and linear.\n",
    "- Additive: The association between a predictor X and response Y does not depend on the valiues of the other predictors. (this is usually not the case)\n",
    "- Linear: As X increases by 1 unit, Y always changes the same.\n",
    "\n",
    "### Removing the Additive Assumption\n",
    "- standard linear model with 2 variables: Y = a + bx1 + cx2 + e\n",
    "- add an interaction term\n",
    "    - Y = a + bx1 + cx2 + dx1x2 + e\n",
    "- we can also haeve interactive term for qualitative and quantitative variables.\n",
    "\n",
    "### Remvoing the linear Assumption\n",
    "- we can create a variable for the polynomial\n",
    "- y = a + bx1 + cx1^2 + e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84373d",
   "metadata": {},
   "source": [
    "### Correlation of Error Terms\n",
    "- we assume that the error terms are not correlated, however if they were then we would be underestimating them and the actual 95% confidence interval would be larger than the one we calulated.\n",
    "\n",
    "### Non-constant Variance of Error terms\n",
    "- Another assumption of linear regression model is that the error terms have a constant variance, Var(ei) = o^2\n",
    "\n",
    "### High Leverage Points (HLP)\n",
    "- outliers: unusual value of y given predictor x.\n",
    "- high leverage points: unsual value of predictor x.\n",
    "- Its harder to see the HLP with more dimensions (parameters).\n",
    "- there is a formula to calculate the leverge.\n",
    "\n",
    "### Collinearity\n",
    "- 2 or more predictor variables are closely related to one another.\n",
    "- VIFj = 1/1-Rj^2\n",
    "    - VIFj = 1: no correlation with others\n",
    "    - >5: moderate collinearity\n",
    "    - >10: severe multicollinearity\n",
    "- it‚Äôs impossible for the regression model to tell which variable is actually responsible for the change in the response y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8bd36",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comparison of Linear Regression and K-Nearest Neighbors\n",
    "This section compares **Linear Regression (parametric)** and **K-Nearest Neighbors (KNN) (non-parametric)** regression methods.\n",
    "\n",
    "Both aim to estimate the relationship:\n",
    "\\[\n",
    "Y = f(X_1, X_2, \\dots, X_p) + \\epsilon\n",
    "\\]\n",
    "but use very different approaches.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "### Concept\n",
    "Assumes the relationship between predictors and response is **linear**:\n",
    "\\[\n",
    "f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
    "\\]\n",
    "\n",
    "### Characteristics\n",
    "- **Parametric**: Specifies a fixed form for \\( f(X) \\)\n",
    "- **Low variance**, **high bias**\n",
    "- Performs well if the true relationship is approximately linear\n",
    "- Easy to interpret, but cannot capture complex nonlinear patterns\n",
    "\n",
    "### Pros\n",
    "‚úÖ Simple and interpretable  \n",
    "‚úÖ Efficient with small data  \n",
    "‚úÖ Works well in high dimensions  \n",
    "\n",
    "### Cons\n",
    "‚ùå High bias if the true relationship is nonlinear  \n",
    "‚ùå Limited flexibility\n",
    "\n",
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Concept\n",
    "A **non-parametric** method: no assumption about the form of \\( f(X) \\).\n",
    "\n",
    "For a test point \\( x_0 \\):\n",
    "1. Identify the **K nearest** observations in the training set.\n",
    "2. Predict:\n",
    "   \\[\n",
    "   \\hat{f}(x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_K(x_0)} y_i\n",
    "   \\]\n",
    "\n",
    "### Characteristics\n",
    "- **Flexible** and can adapt to complex patterns\n",
    "- **Low bias**, **high variance** (especially for small K)\n",
    "- Sensitive to the choice of K and dimensionality of data\n",
    "\n",
    "### Pros\n",
    "‚úÖ Captures nonlinear relationships  \n",
    "‚úÖ No need to specify functional form  \n",
    "\n",
    "### Cons\n",
    "‚ùå Poor interpretability  \n",
    "‚ùå Sensitive to noise and irrelevant features  \n",
    "‚ùå Suffers from the **curse of dimensionality**\n",
    "\n",
    "## ‚öñÔ∏è Bias‚ÄìVariance Tradeoff\n",
    "\n",
    "| Model | Bias | Variance | Flexibility |\n",
    "|--------|------|-----------|--------------|\n",
    "| Linear Regression | High | Low | Low |\n",
    "| KNN (small K) | Low | High | High |\n",
    "| KNN (large K) | Higher | Lower | Less flexible |\n",
    "\n",
    "**Goal:** Minimize the **Test Mean Squared Error (MSE)** by balancing bias and variance.\n",
    "\n",
    "\\[\n",
    "\\text{Test MSE} = [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\hat{f}(x_0)) + \\text{Var}(\\epsilon)\n",
    "\\]\n",
    "\n",
    "## üìä Performance Insights\n",
    "\n",
    "- **Linear Regression**: better when the true relationship is roughly linear or data are limited.  \n",
    "- **KNN Regression**: better when the relationship is complex and nonlinear, and you have lots of data.  \n",
    "- In **high dimensions**, KNN performs poorly due to sparse data (curse of dimensionality).\n",
    "- As p increases the MSE for KNN significantly increases"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
